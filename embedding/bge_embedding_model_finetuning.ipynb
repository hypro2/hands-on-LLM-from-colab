{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MT1eAv_MI97w",
        "outputId": "82fafcde-1312-494a-e1dc-cacb71afd390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FlagEmbedding\n",
            "  Using cached FlagEmbedding-1.3.4.tar.gz (163 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting peft\n",
            "  Using cached peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting LM_Cocktail\n",
            "  Downloading LM_Cocktail-0.0.4.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.44.2 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (4.51.1)\n",
            "Collecting datasets>=2.19.0 (from FlagEmbedding)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate>=0.20.1 in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (1.5.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (3.4.1)\n",
            "Collecting ir-datasets (from FlagEmbedding)\n",
            "  Downloading ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from FlagEmbedding) (5.29.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.19.0->FlagEmbedding)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.19.0->FlagEmbedding)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.19.0->FlagEmbedding)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.19.0->FlagEmbedding)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.19.0->FlagEmbedding) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->FlagEmbedding)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->FlagEmbedding) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2->FlagEmbedding) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.44.2->FlagEmbedding) (0.21.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (4.13.3)\n",
            "Collecting inscriptis>=2.2.0 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->FlagEmbedding) (5.3.2)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir-datasets->FlagEmbedding)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers->FlagEmbedding) (11.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->FlagEmbedding) (2.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.19.0->FlagEmbedding) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.19.0->FlagEmbedding) (2025.1.31)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir-datasets->FlagEmbedding)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.19.0->FlagEmbedding) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.0->FlagEmbedding) (1.17.0)\n",
            "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: FlagEmbedding, LM_Cocktail, warc3-wet-clueweb09, cbor\n",
            "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.3.4-py3-none-any.whl size=232500 sha256=ea65939c60f18c068c0570809e4f36876cd8788d61e78c0ea424a69364c41c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/69/8b/bb209413e16cb21065716300c291e75ca5949c878283836c4d\n",
            "  Building wheel for LM_Cocktail (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for LM_Cocktail: filename=LM_Cocktail-0.0.4-py3-none-any.whl size=9624 sha256=889ddbd4aa2fa612d8bc11ebbdd8cd1ecf0f880ba1ce85acd8279ac5c2576d0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/58/d0/036177e687d42e35cc67d11191f45c7211f2693fe089bff6f8\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=d01e9b6df00d0dfa08bc70c7faafa3c57362bc5009cbbd893bfc1f0469ace36f\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53930 sha256=cdd96ad2fd2bee0b5893432b056ebbc1adaef4c3c3d69ab5936f2132e0036a35\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built FlagEmbedding LM_Cocktail warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, xxhash, unlzw3, trec-car-tools, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, fsspec, faiss-cpu, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, inscriptis, nvidia-cusolver-cu12, ir-datasets, datasets, peft, LM_Cocktail, FlagEmbedding\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed FlagEmbedding-1.3.4 LM_Cocktail-0.0.4 cbor-1.0.0 datasets-3.5.0 dill-0.3.8 faiss-cpu-1.10.0 fsspec-2024.12.0 ijson-3.3.0 inscriptis-2.6.0 ir-datasets-0.5.10 lz4-4.4.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.1 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 xxhash-3.5.0 zlib-state-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install -U FlagEmbedding peft faiss-cpu LM_Cocktail"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PFIwvPh-SbGP",
        "outputId": "7e722a80-7678-47ff-a1a9-62868028009a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.11/dist-packages (0.16.5)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.11/dist-packages (2.7.4.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FlagOpen/FlagEmbedding.git\n",
        "# !pip install -U FlagEmbedding[finetune]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "araywzdgKHr8",
        "outputId": "a71928c3-eaee-437d-cde2-08239eef2134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FlagEmbedding'...\n",
            "remote: Enumerating objects: 10816, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 10816 (delta 177), reused 211 (delta 148), pack-reused 10536 (from 2)\u001b[K\n",
            "Receiving objects: 100% (10816/10816), 38.90 MiB | 23.13 MiB/s, done.\n",
            "Resolving deltas: 100% (5967/5967), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "우선 학습을 위해 임의의 데이터셋을 준비합니다. 데이터셋의 형식은 반드시 jsonl 형태여야만 합니다. 각 데이터는 \"query\", \"pos\", \"neg\"로 구성되며, 이때 \"query\"는 검색어, \"pos\"는 해당 검색어와 연관된 문서들이 포함된 리스트, \"neg\"는 해당 검색어와 연관되지 않은 문서들이 포함된 리스트입니다.\n",
        "\n",
        "여기서는 저자가 임의로 데이터를 준비했지만, 실제 파인 튜닝 상황에서는 사용자는 실제로 이런 검색어가 입력되었을 때(query), 이런 문서들이 검색되었으면 하며(pos), 이런 문서들은 검색되지 않았으면 한다(neg)에 해당하는 데이터들을 직접 구축해야만 합니다."
      ],
      "metadata": {
        "id": "6GTWsf80WWcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 번역된 데이터\n",
        "translated_data = [\n",
        "    {\"query\": \"다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.\", \"pos\": [\"플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다\"], \"neg\": [\"4명의 여성이 해변에 앉아 있다.\", \"1996년에 개혁이 있었다.\", \"그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.\", \"그 남자는 하와이에 대해 이야기하고 있다.\", \"한 여성이 밖에 서 있다.\", \"전투는 끝났다.\", \"한 무리의 사람들이 배구를 하고 있다.\"]},\n",
        "    {\"query\": \"한 여성이 높은 절벽 위에서 한 발로 서서 강을 내려다보고 있다.\", \"pos\": [\"한 여성이 절벽 위에 서 있다.\"], \"neg\": [\"한 여성이 의자에 앉아 있다.\", \"조지 부시는 공화당원들에게 최고 고문들의 조언에 반하여 이 어리석은 생각을 고려조차 하지 않겠다고 말했다.\", \"그 가족은 무너지고 있었다.\", \"아무도 회의에 나타나지 않았다\", \"한 소년이 밖에서 모래를 가지고 놀고 있다.\", \"전보를 받자마자 끝났다.\", \"한 아이가 자기 방에서 책을 읽고 있다.\"]},\n",
        "    {\"query\": \"두 여성이 악기를 연주하고 있다; 한 명은 클라리넷, 다른 한 명은 바이올린을 연주한다.\", \"pos\": [\"몇 사람이 곡을 연주하고 있다.\"], \"neg\": [\"두 여성이 기타와 드럼을 연주하고 있다.\", \"한 남자가 산을 스키를 타고 내려가고 있다.\", \"살인자가 생각했던 때에 치명적인 용량이 투여되지 않았다.\", \"자전거를 타고 있는 사람\", \"그 소녀는 아치길에 기대어 서 있다.\", \"한 무리의 여성들이 소파 오페라를 보고 있다.\", \"사람들은 나이가 들어도 절대 잊지 않는다.\"]},\n",
        "    {\"query\": \"파란색 탱크톱을 입은 소녀가 앉아서 세 마리의 개를 지켜보고 있다.\", \"pos\": [\"한 소녀가 파란색을 입고 있다.\"], \"neg\": [\"한 소녀가 세 마리의 고양이와 함께 있다.\", \"사람들이 장례 행렬을 지켜보고 있다.\", \"그 아이는 검은색을 입고 있다.\", \"공립학교에서 우리에게 재정은 문제이다.\", \"수영장에 있는 아이들.\", \"폭행당하는 것은 진정시키는 일이다.\", \"나는 18살에 심각한 문제에 직면했다.\"]},\n",
        "    {\"query\": \"노란 개가 숲길을 따라 달리고 있다.\", \"pos\": [\"개가 달리고 있다\"], \"neg\": [\"고양이가 달리고 있다\", \"스틸은 그녀의 원래 이야기를 지키지 않았다.\", \"이 규칙은 사람들이 자녀 양육비를 내는 것을 막는다.\", \"조끼를 입은 남자가 차 안에 앉아 있다.\", \"검은 옷을 입고 흰색 반다나와 선글라스를 낀 사람이 버스 정류장에서 기다리고 있다.\", \"글로브나 메일 중 어느 쪽도 캐나다의 현재 도로 체계 상태에 대해 언급하지 않았다.\", \"스프링 크릭 시설은 오래되고 구식이다.\"]},\n",
        "    {\"query\": \"각 단계에서의 필수 활동과 그 활동들과 관련된 중요한 요소들을 설명한다.\", \"pos\": [\"필수 활동에 대한 중요 요소들이 설명되어 있다.\"], \"neg\": [\"중요한 활동들을 설명하지만 그 활동들과 관련된 중요한 요소들에 대한 규정은 없다.\", \"사람들이 항의하기 위해 모여 있다.\", \"주 정부는 당신이 그렇게 하기를 선호할 것이다.\", \"한 소녀가 한 소년 옆에 앉아 있다.\", \"두 남성이 공연하고 있다.\", \"아무도 뛰고 있지 않다\", \"콘라드는 머리를 맞도록 음모를 꾸미고 있었다.\"]},\n",
        "    {\"query\": \"한 남자가 레스토랑에서 연설을 하고 있다.\", \"pos\": [\"한 사람이 연설을 하고 있다.\"], \"neg\": [\"그 남자는 테이블에 앉아 음식을 먹고 있다.\", \"이것은 확실히 승인이 아니다.\", \"그들은 은퇴 때문에 집을 팔았지, 대출 때문이 아니다.\", \"미주리 주의 인장은 완벽하다.\", \"누군가가 손을 들고 있다.\", \"한 운동선수가 1500미터 수영 경기에 참가하고 있다.\", \"두 남자가 마술 쇼를 보고 있다.\"]},\n",
        "    {\"query\": \"인디언들이 코트를 입고 음식과 음료를 가지고 모임을 갖고 있다.\", \"pos\": [\"인디언 그룹이 음식과 음료를 가지고 모임을 갖고 있다\"], \"neg\": [\"인디언 그룹이 장례식을 하고 있다\", \"이것은 팔마의 큰 투우장에서 겨울 오후에만 공연된다.\", \"올바른 정보는 법률 서비스 관행과 사법 체계를 강화할 수 있다.\", \"한편, 본토는 인구가 없었다.\", \"두 아이가 자고 있다.\", \"어부가 원숭이를 잡으려고 하고 있다\", \"사람들이 기차 안에 있다\"]},\n",
        "    {\"query\": \"보라색 머리를 한 여성이 밖에서 자전거를 타고 있다.\", \"pos\": [\"한 여성이 자전거를 타고 있다.\"], \"neg\": [\"한 여성이 공원에서 조깅을 하고 있다.\", \"그 거리는 하얀색으로 칠해진 집들로 가득했다.\", \"한 그룹이 안에서 영화를 보고 있다.\", \"소풍에서 남자들이 스테이크를 자르고 있다\", \"여러 명의 요리사들이 앉아서 음식에 대해 이야기하고 있다.\", \"위원회는 중요한 대안들이 고려되지 않았다고 지적한다.\", \"우리는 장작이 다 떨어져서 불을 위해 소나무 바늘을 사용해야 했다.\"]},\n",
        "    {\"query\": \"한 남자가 도시 거리에서 인력거로 두 여성을 끌고 있다.\", \"pos\": [\"한 남자가 도시에 있다.\"], \"neg\": [\"한 남자가 비행기 조종사이다.\", \"그것은 지루하고 평범하다.\", \"아침 햇살이 밝게 비치고 따뜻했다.\", \"두 사람이 부두에서 뛰어내렸다.\", \"사람들이 우주선 발사를 보고 있다.\", \"테레사 수녀는 쉬운 선택이다.\", \"원하는 속도로 갈 수 있는 것은 가치가 있다.\"]}\n",
        "]\n",
        "\n",
        "# JSONL 파일로 저장\n",
        "with open('toy_finetune_data.jsonl', 'w', encoding='utf-8') as f:\n",
        "    for item in translated_data:\n",
        "        json.dump(item, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"데이터가 'toy_finetune_data.jsonl' 파일로 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNZ3tT2pJJix",
        "outputId": "99882448-427f-419f-d9fa-646dafa16743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터가 'toy_finetune_data.jsonl' 파일로 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 자동으로 최적화하는 과정을 거쳐봅시다.\n",
        "\n",
        "!python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\\n",
        "--model_name_or_path BAAI/bge-m3 \\\n",
        "--input_file toy_finetune_data.jsonl \\\n",
        "--output_file toy_finetune_data_minedHN.jsonl \\\n",
        "--range_for_sampling 2-200 \\\n",
        "--negative_number 15 \\\n",
        "--use_gpu_for_searching\n",
        "\n",
        "\n",
        "input_file: 최적화 하기를 원하는 여러분들이 파인튜닝을 위해 준비한 JSON 데이터입니다. 위 코드를 실행하면 각 query에 대해서 실제로 bge-m3로 유사도가 높은 상위 k개의 문서를 검색하고, 이 상위 k개 문서에서 무작위로 네거티브 샘플링을 수행합니다 (pos 문서는 제외).\n",
        "\n",
        "output_file: 네거티브 샘플링을 수행하여 여러분들의 데이터를 학습하기에 최적화 시킨 후의 JSON 데이터를 저장할 경로입니다.\n",
        "\n",
        "negative_number: 샘플링할 네거티브의 수입니다.\n",
        "\n",
        "range_for_sampling: 네거티브를 샘플링할 범위입니다. 예를 들어, 2-100은 상위 2위부터 200위 문서 중에서 negative_number만큼의 네거티브를 샘플링한다는 의미입니다. 네거티브의 난이도를 낮추기 위해 더 큰 값을 설정할 수 있습니다 (예: 상위 60-300위 문서에서 네거티브를 샘플링하려면 60-300으로 설정)\n",
        "\n",
        "use_gpu_for_searching: 네거티브 검색에 faiss-gpu를 사용할지 여부입니다."
      ],
      "metadata": {
        "id": "M7OPrU0tWSUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/FlagEmbedding/research/baai_general_embedding/finetune/hn_mine.py \\\n",
        "--model_name_or_path BAAI/bge-m3 \\\n",
        "--input_file toy_finetune_data.jsonl \\\n",
        "--output_file toy_finetune_data_minedHN.jsonl \\\n",
        "--range_for_sampling 2-200 \\\n",
        "--negative_number 15 \\\n",
        "# --use_gpu_for_searching\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWhttskcJL3z",
        "outputId": "7034fc16-8db6-4674-f7f7-4d541b5501fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-15 02:01:36.653121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744682496.674244    3650 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744682496.681561    3650 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-15 02:01:36.703717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-04-15 02:01:41,564] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "inferencing embedding for corpus (number=80)--------------\n",
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "inferencing embedding for queries (number=10)--------------\n",
            "create index and search------------------\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7b1a9816a200>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "AttributeError: 'NoneType' object has no attribute '_xla'\n",
            "Exception ignored in: <function _xla_gc_callback at 0x7b1a9816a200>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "AttributeError: 'NoneType' object has no attribute '_xla'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "with open('toy_finetune_data.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        data.append(item)\n",
        "\n",
        "data[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNFNhzmjL4F1",
        "outputId": "abb184ea-76fb-4171-cd70-3cf6cd391b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': '다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.',\n",
              " 'pos': ['플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다'],\n",
              " 'neg': ['4명의 여성이 해변에 앉아 있다.',\n",
              "  '1996년에 개혁이 있었다.',\n",
              "  '그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.',\n",
              "  '그 남자는 하와이에 대해 이야기하고 있다.',\n",
              "  '한 여성이 밖에 서 있다.',\n",
              "  '전투는 끝났다.',\n",
              "  '한 무리의 사람들이 배구를 하고 있다.']}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "with open('toy_finetune_data_minedHN.jsonl', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        item = json.loads(line)\n",
        "        data.append(item)\n",
        "\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjAHqzCBME3l",
        "outputId": "cfc10f95-958d-49ec-dd5c-7aa313bb105b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': '다섯 명의 여성이 해변을 따라 플립플롭을 신고 걸어간다.',\n",
              " 'pos': ['플립플롭을 신은 몇몇 여성들이 해변을 따라 걸어가고 있다'],\n",
              " 'neg': ['한 아이가 자기 방에서 책을 읽고 있다.',\n",
              "  '원하는 속도로 갈 수 있는 것은 가치가 있다.',\n",
              "  '사람들이 기차 안에 있다',\n",
              "  '살인자가 생각했던 때에 치명적인 용량이 투여되지 않았다.',\n",
              "  '이것은 확실히 승인이 아니다.',\n",
              "  '이것은 팔마의 큰 투우장에서 겨울 오후에만 공연된다.',\n",
              "  '두 여성이 기타와 드럼을 연주하고 있다.',\n",
              "  '한 소년이 밖에서 모래를 가지고 놀고 있다.',\n",
              "  '한 그룹이 안에서 영화를 보고 있다.',\n",
              "  '두 아이가 자고 있다.',\n",
              "  '위원회는 중요한 대안들이 고려되지 않았다고 지적한다.',\n",
              "  '그녀는 자신의 기록을 정정하기 위해 법정에 가지 않을 것이다.',\n",
              "  '콘라드는 머리를 맞도록 음모를 꾸미고 있었다.',\n",
              "  '검은 옷을 입고 흰색 반다나와 선글라스를 낀 사람이 버스 정류장에서 기다리고 있다.',\n",
              "  '한 여성이 자전거를 타고 있다.']}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "per_device_train_batch_size: 학습 시 배치 크기입니다. 대부분의 경우, 더 큰 배치 크기가 더 강력한 성능을 가져옵니다. GPU 성능에 달려있으며, 여기서는 토이 프로젝트이고 데이터가 소량이므로 1을 선택했습니다.\n",
        "\n",
        "train_group_size: 학습 시 쿼리당 긍정 및 부정 예제의 수입니다. 항상 하나의 긍정 예제가 있으므로, 이 인자는 부정 예제의 수를 제어합니다 (부정 예제 수 = train_group_size - 1). 부정 예제의 수가 데이터의 \"neg\":List[str]에 있는 부정 예제 수보다 크지 않아야 함에 주의하세요. 이 그룹의 부정 예제 외에도, 배치 내 부정 예제도 파인튜닝에 사용됩니다.\n",
        "\n",
        "negatives_cross_device: 모든 GPU에서 부정 예제를 공유합니다. 이 인자는 부정 예제의 수를 확장합니다.\n",
        "\n",
        "learning_rate: 모델에 적합한 값을 선택하세요. 대규모/기본/소규모 모델에 대해 각각 1e-5/2e-5/3e-5를 추천합니다.\n",
        "\n",
        "temperature: 유사도 점수의 분포에 영향을 미칩니다. 권장 값: 0.01-0.1.\n",
        "\n",
        "query_max_len: 쿼리의 최대 길이입니다. 데이터의 평균 쿼리 길이에 따라 설정해 주세요.\n",
        "\n",
        "passage_max_len: 문장의 최대 길이입니다. 데이터의 평균 문장 길이에 따라 설정해 주세요.\n",
        "\n",
        "query_instruction_for_retrieval: 쿼리에 대한 지시사항으로, 각 쿼리에 추가됩니다. 아무것도 추가하지 않으려면 \"\"로 설정할 수 있습니다.\n",
        "\n",
        "use_inbatch_neg: 같은 배치 내의 문장들을 부정 예제로 사용합니다. 기본값은 True입니다.\n",
        "\n",
        "save_steps: 체크포인트를 저장할 학습 단계 간격을 설정합니다.\n",
        "학습이 끝나면 checkpoint라는 디렉토리에 학습 모델이 저장됩니다."
      ],
      "metadata": {
        "id": "gux8DK7hWbs7"
      }
    },
    {
      "source": [
        "!torchrun --nproc_per_node 1 \\\n",
        "    -m FlagEmbedding.finetune.embedder.encoder_only.base \\\n",
        "    --model_name_or_path BAAI/bge-large-en-v1.5 \\\n",
        "    --cache_dir ./cache/model \\\n",
        "    --train_data /content/toy_finetune_data.jsonl \\\n",
        "    --cache_path ./cache/data \\\n",
        "    --train_group_size 1 \\\n",
        "    --query_max_len 512 \\\n",
        "    --passage_max_len 512 \\\n",
        "    --pad_to_multiple_of 8 \\\n",
        "    --query_instruction_for_retrieval 'Represent this sentence for searching relevant passages: ' \\\n",
        "    --query_instruction_format '{}{}' \\\n",
        "    --knowledge_distillation False \\\n",
        "    --output_dir ./test_encoder_only_base_bge-m3 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --fp16 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --dataloader_drop_last True \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --gradient_checkpointing \\\n",
        "    --logging_steps 1 \\\n",
        "    --save_steps 1000 \\\n",
        "    --negatives_cross_device \\\n",
        "    --temperature 0.02 \\\n",
        "    --sentence_pooling_method cls \\\n",
        "    --normalize_embeddings True \\\n",
        "    --kd_loss_type kl_div \\\n",
        "    --deepspeed /content/FlagEmbedding/FlagEmbedding/finetune/ds_stage0.json"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mKMOQ8yOiDz",
        "outputId": "0b1e98f7-a2c7-4235-e11c-48c7cd327434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-15 02:44:13.345712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744685053.367708   15829 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744685053.374219   15829 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-04-15 02:44:16,955] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-04-15 02:44:18,121] [INFO] [comm.py:658:init_distributed] cdb=None\n",
            "[2025-04-15 02:44:18,121] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "04/15/2025 02:44:18 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
            "04/15/2025 02:44:18 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Training/evaluation parameters AbsEmbedderTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=/content/FlagEmbedding/FlagEmbedding/finetune/ds_stage0.json,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.NO,\n",
            "eval_use_gather_object=False,\n",
            "fix_position_embedding=False,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "kd_loss_type=kl_div,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./test_encoder_only_base_bge-m3/runs/Apr15_02-44-18_33ac7e439cfb,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1.0,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "negatives_cross_device=True,\n",
            "no_cuda=False,\n",
            "normalize_embeddings=True,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./test_encoder_only_base_bge-m3,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./test_encoder_only_base_bge-m3,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sentence_pooling_method=cls,\n",
            "skip_memory_metrics=True,\n",
            "sub_batch_size=None,\n",
            "temperature=0.02,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tp_size=0,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "04/15/2025 02:44:18 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Model parameters AbsEmbedderModelArguments(model_name_or_path='BAAI/bge-large-en-v1.5', config_name=None, tokenizer_name=None, cache_dir='./cache/model', trust_remote_code=False, token=None)\n",
            "04/15/2025 02:44:18 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Data parameters AbsEmbedderDataArguments(train_data=['/content/toy_finetune_data.jsonl'], cache_path='./cache/data', train_group_size=1, query_max_len=512, passage_max_len=512, pad_to_multiple_of=8, max_example_num_per_dataset=100000000, query_instruction_for_retrieval='Represent this sentence for searching relevant passages: ', query_instruction_format='{}{}', knowledge_distillation=False, passage_instruction_for_retrieval=None, passage_instruction_format='{}{}', shuffle_ratio=0.0, same_dataset_within_batch=False, small_threshold=0, drop_threshold=0)\n",
            "tokenizer_config.json: 100% 366/366 [00:00<00:00, 2.30MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 1.74MB/s]\n",
            "tokenizer.json: 100% 711k/711k [00:00<00:00, 3.43MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 1.09MB/s]\n",
            "config.json: 100% 779/779 [00:00<00:00, 6.75MB/s]\n",
            "model.safetensors: 100% 1.34G/1.34G [00:19<00:00, 69.5MB/s]\n",
            "04/15/2025 02:44:40 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.runner -   Config: BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/15/2025 02:44:40 - INFO - FlagEmbedding.abc.finetune.embedder.AbsDataset -   loading data from /content/toy_finetune_data.jsonl ...\n",
            "/usr/local/lib/python3.11/dist-packages/FlagEmbedding/finetune/embedder/encoder_only/base/runner.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = EncoderOnlyEmbedderTrainer(\n",
            "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/fused_adam/build.ninja...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.029216766357421875 seconds\n",
            "[2025-04-15 02:44:42,457] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "  0% 0/10 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.1}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'epoch': 0.2}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1e-05, 'epoch': 0.3}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.4}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.5}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 6.25e-06, 'epoch': 0.6}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 5e-06, 'epoch': 0.7}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.8}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 2.5e-06, 'epoch': 0.9}\n",
            "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.25e-06, 'epoch': 1.0}\n",
            "100% 10/10 [00:03<00:00,  3.19it/s]04/15/2025 02:44:51 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.trainer -   Saving model checkpoint to ./test_encoder_only_base_bge-m3/checkpoint-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 BGE 모델을 파인튜닝하면 목표 작업에서의 성능을 향상시킬 수 있지만, 학습한 유형의 도메인을 넘어선 모델의 일반적인 능력이 저하될 수도 있습니다. 예를 들어서 금융 분야의 도메인의 데이터를 학습시키면, 금융 분야에서는 성능이 오를지 몰라도 그 외의 도메인의 데이터에서는 성능 저하가 심각하게 발생하는 경우 등이 있습니다. 이 경우, 파인 튜닝 모델과 기존 모델을 병합하여 일반화 성능을 어느 정도 유지하면서도 파인 튜닝 모델 성능을 가져올 수 있습니다.\n",
        "\n",
        "두 모델 중 어떤 모델의 능력을 더 우선시 하면서 융합할 것인지를 정할 수 있는데, 이는 아래의 코드에서 weights의 값을 조절하여 결정합니다. 예컨대 [0.5, 0.5]는 두 모델의 비중을 50:50으로 가져가는 것을 의미합니다."
      ],
      "metadata": {
        "id": "iGZLX2tTnEr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from LM_Cocktail import mix_models, mix_models_with_data\n",
        "\n",
        "# 기존 모델과 파인 튜닝 모달을 융합하여 mixed_model_1 디렉토리에 융합 모델을 저장.\n",
        "model = mix_models(\n",
        "    model_names_or_paths=[\"BAAI/bge-m3\", \"/content/checkpoint\"],\n",
        "    model_type='encoder',\n",
        "    weights=[0.5, 0.5],  # 가중치를 조절하세요.\n",
        "    output_path='./mixed_model_1')"
      ],
      "metadata": {
        "id": "1urccy_kUA0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5b23mcKjUA2v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}