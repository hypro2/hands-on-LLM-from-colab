{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXsGHmqy9opXxxBehl1VzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e11246421ae4d879dbdc265a1c2e6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02edf37dbda64c2eae5f788da34af6a3",
              "IPY_MODEL_d6b23302369e4c81bd74da6359f665d0",
              "IPY_MODEL_6de96570a75e4b69ab4f231e581fe757"
            ],
            "layout": "IPY_MODEL_5774dbd7becb498284399ee41313ac74"
          }
        },
        "02edf37dbda64c2eae5f788da34af6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecc5b99d96842608b6cfbe73de71120",
            "placeholder": "​",
            "style": "IPY_MODEL_2978dcc8be9e4b1d86f0791fcb968211",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d6b23302369e4c81bd74da6359f665d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d749341836d4713bf4564ed1d8ddc31",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_779e4127427047b49759840e26604691",
            "value": 4
          }
        },
        "6de96570a75e4b69ab4f231e581fe757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_489a877871854ce982ec2e3ed4f266a8",
            "placeholder": "​",
            "style": "IPY_MODEL_8e82d48d166f44dcae23e661a30d1891",
            "value": " 4/4 [01:13&lt;00:00, 15.17s/it]"
          }
        },
        "5774dbd7becb498284399ee41313ac74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecc5b99d96842608b6cfbe73de71120": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2978dcc8be9e4b1d86f0791fcb968211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d749341836d4713bf4564ed1d8ddc31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779e4127427047b49759840e26604691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "489a877871854ce982ec2e3ed4f266a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e82d48d166f44dcae23e661a30d1891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypro2/hands-on-LLM-from-colab/blob/main/LLM2VEC_LoRA_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머\n",
        "!pip install -q transformers==4.44.2\n",
        "\n",
        "# 양자화\n",
        "!pip install -U -q accelerate bitsandbytes peft\n",
        "\n",
        "# 임베딩\n",
        "!pip install -q llm2vec\n",
        "!pip install -q flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "eUZrKvQvH0x3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a9c90db3-108c-4f26-f063-445018f25c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.2.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acess Token 입력\n",
        "\n",
        "https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "2SPLBVdmHWF1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooRE1aiWG_JX",
        "outputId": "be087dc6-b9e3-41af-d2b7-fdcb08c9d7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_TOKEN:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF_TOKEN:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "4k8AsebgONWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, LlamaPreTrainedModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
        "from functools import lru_cache, partial, wraps"
      ],
      "metadata": {
        "id": "TZA2b6Z3KdNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import BCEWithLogitsLoss, MSELoss\n",
        "\n",
        "def fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs):\n",
        "    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n",
        "    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n",
        "    if reduction == \"sum\":\n",
        "        loss = loss / num_items_in_batch\n",
        "    return loss"
      ],
      "metadata": {
        "id": "xf5EWexBTGqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ForSequenceClassificationLoss(labels, pooled_logits, config, **kwargs):\n",
        "    num_labels = config.num_labels\n",
        "    if config.problem_type is None:\n",
        "        if num_labels == 1:\n",
        "            config.problem_type = \"regression\"\n",
        "        elif num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "            config.problem_type = \"single_label_classification\"\n",
        "        else:\n",
        "            config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "    if config.problem_type == \"regression\":\n",
        "        loss_fct = MSELoss()\n",
        "        if num_labels == 1:\n",
        "            loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
        "        else:\n",
        "            loss = loss_fct(pooled_logits, labels)\n",
        "    elif config.problem_type == \"single_label_classification\":\n",
        "        loss = fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)\n",
        "    elif config.problem_type == \"multi_label_classification\":\n",
        "        loss_fct = BCEWithLogitsLoss()\n",
        "        loss = loss_fct(pooled_logits, labels)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "TKHxGIm9SyN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaBiForSequenceClassification(LlamaPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.model = LlamaBiModel(config)\n",
        "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        transformer_outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "        logits = self.score(hidden_states)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            batch_size = input_ids.shape[0]\n",
        "        else:\n",
        "            batch_size = inputs_embeds.shape[0]\n",
        "\n",
        "        if self.config.pad_token_id is None and batch_size != 1:\n",
        "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
        "        if self.config.pad_token_id is None:\n",
        "            sequence_lengths = -1\n",
        "        else:\n",
        "            if input_ids is not None:\n",
        "                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n",
        "                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n",
        "                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n",
        "                sequence_lengths = sequence_lengths.to(logits.device)\n",
        "            else:\n",
        "                sequence_lengths = -1\n",
        "\n",
        "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = ForSequenceClassificationLoss(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (pooled_logits,) + transformer_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=pooled_logits,\n",
        "            past_key_values=transformer_outputs.past_key_values,\n",
        "            hidden_states=transformer_outputs.hidden_states,\n",
        "            attentions=transformer_outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "3rnOnlctKQQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, LlamaPreTrainedModel\n",
        "from llm2vec.models.bidirectional_llama import LlamaBiModel\n",
        "from peft import PeftModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"WIPSAI/Llama-3-8B-Patent-Embedding\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"WIPSAI/Llama-3-8B-Patent-Embedding\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDiDss_SMOPa",
        "outputId": "3093262f-53be-47cf-9f3e-a9fc677eead9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifer_model = LlamaBiForSequenceClassification.from_pretrained(\"WIPSAI/Llama-3-8B-Patent-Embedding\",\n",
        "                                                                   num_labels=2,\n",
        "                                                                   load_in_4bit=True,\n",
        "                                                                   device_map='auto',\n",
        "                                                                   torch_dtype=\"auto\")\n",
        "classifer_model.config.pad_token_id = 128001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "7e11246421ae4d879dbdc265a1c2e6bc",
            "02edf37dbda64c2eae5f788da34af6a3",
            "d6b23302369e4c81bd74da6359f665d0",
            "6de96570a75e4b69ab4f231e581fe757",
            "5774dbd7becb498284399ee41313ac74",
            "cecc5b99d96842608b6cfbe73de71120",
            "2978dcc8be9e4b1d86f0791fcb968211",
            "1d749341836d4713bf4564ed1d8ddc31",
            "779e4127427047b49759840e26604691",
            "489a877871854ce982ec2e3ed4f266a8",
            "8e82d48d166f44dcae23e661a30d1891"
          ]
        },
        "id": "xlEKt-roGHIu",
        "outputId": "670151ae-996e-4ce1-8ae9-677eb89b0882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e11246421ae4d879dbdc265a1c2e6bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaBiForSequenceClassification were not initialized from the model checkpoint at WIPSAI/Llama-3-8B-Patent-Embedding and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA 설정\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Low-rank의 차원\n",
        "    lora_alpha=16,  # LoRA alpha\n",
        "    lora_dropout=0.1,  # Dropout 비율\n",
        "    task_type=\"SEQ_CLS\",  # 시퀀스 분류\n",
        "    target_modules=[\"q_proj\", \"k_proj\",\"v_proj\",\"o_proj\"],\n",
        ")\n",
        "classifer_model = get_peft_model(classifer_model, lora_config)"
      ],
      "metadata": {
        "id": "p-qrQsGGGz_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for parameter in classifer_model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "for parameter in classifer_model.score.parameters():\n",
        "    parameter.requires_grad = True"
      ],
      "metadata": {
        "id": "4oLjbeVbGs8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifer_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La1j7mpZG1-Z",
        "outputId": "15d24bb0-0f59-4d7f-8486-f460ebad9975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaBiForSequenceClassification(\n",
              "      (model): LlamaBiModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x ModifiedLlamaDecoderLayer(\n",
              "            (self_attn): ModifiedLlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=2, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(example, tokenizer):\n",
        "    return tokenizer(example[\"document\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset(\"nsmc\", trust_remote_code=True)\n",
        "\n",
        "processed_dataset = dataset.map(\n",
        "    lambda example: preprocess_data(example, tokenizer),\n",
        "    batched=True,\n",
        "    remove_columns=[\"id\", \"document\"]\n",
        ").rename_column(\"label\", \"labels\")\n",
        "\n",
        "print(dataset)\n",
        "print(processed_dataset)\n",
        "print(dataset[\"train\"][0])\n",
        "print(processed_dataset[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LX6DMTAG5DL",
        "outputId": "ade144c4-b525-4424-c71b-c2d2bb80070a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'document', 'label'],\n",
            "        num_rows: 150000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'document', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 150000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['labels', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}\n",
            "{'labels': 0, 'input_ids': [128000, 54059, 102519, 126015, 497, 118769, 49011, 250, 102249, 61415, 108231, 121279, 29102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "\n",
        "longest_collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"longest\"\n",
        ")\n",
        "longest_dataloader = DataLoader(\n",
        "    processed_dataset[\"train\"],\n",
        "    collate_fn=longest_collator,\n",
        "    batch_size=4,\n",
        "    shuffle=False\n",
        ")\n",
        "longest_iterator = iter(longest_dataloader)\n",
        "longest_batch = next(longest_iterator)\n",
        "print(\"longest 패딩 입력 id shape :\", longest_batch[\"input_ids\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDZFxPtLG9OP",
        "outputId": "c34f9066-8190-4eaf-a70e-e52d545c29fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "longest 패딩 입력 id shape : torch.Size([4, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"text-classification\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=1,\n",
        "    eval_steps=200,\n",
        "    logging_steps=200,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=classifer_model,\n",
        "    args=training_args,\n",
        "    data_collator=longest_collator,\n",
        "    train_dataset=processed_dataset[\"train\"].select(range(10000)),\n",
        "    eval_dataset=processed_dataset[\"test\"].select(range(100))\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "ih3UqE-oHBsv",
        "outputId": "71597bd6-e756-4758-c8d3-14e0beed2599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[\u001b[0m (\u001b[-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241031_064110-ct2gdita</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/-personal/huggingface/runs/ct2gdita' target=\"_blank\">forsaken-silence-9</a></strong> to <a href='https://wandb.ai/-personal/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/-personal/huggingface' target=\"_blank\">https://wandb.ai/-personal/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/-personal/huggingface/runs/ct2gdita' target=\"_blank\">https://wandb.ai/-personal/huggingface/runs/ct2gdita</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='610' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 610/1250 34:00 < 35:47, 0.30 it/s, Epoch 0.49/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.954500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.904700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Js8fy-WHZS1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
